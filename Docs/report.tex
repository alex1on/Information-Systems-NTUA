\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
% \usepackage{natbib}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\definecolor{linkblue}{RGB}{0,0,255}
\usepackage{hyperref}
\usepackage{amssymb}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Distributed Execution of SQL Queries over Trino \\
	% {\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
	% should not be used}
	\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{Alexandros Ionitsa}
	\IEEEauthorblockA{\textit{Dept. of Electrical \& Computer Engineering} \\
		\textit{National Technical University of Athens}\\
		Athens, Greece \\
		el19193@mail.ntua.gr}
	\and
	\IEEEauthorblockN{Emmanouil Emmanouilidis}
	\IEEEauthorblockA{\textit{Dept. of Electrical \& Computer Engineering} \\
		\textit{National Technical University of Athens}\\
		Athens, Greece \\
		el19435@mail.ntua.gr}
}

\maketitle

\begin{abstract}

	The explosion of data in our world has made analyzing large datasets—commonly known as big data—a
	crucial factor for competition and productivity growth. This exponential growth has necessitated
	innovative solutions for efficient data processing. Trino, formerly known as PrestoDB \cite{b1}, \cite{b2}, emerges as
	a pivotal tool designed for distributed SQL queries across large and diverse data sets. In this
	study, we assess the performance of Trino in various scenarios, such as different data distribution
	strategies,  worker configurations, and query complexities. For this project, we utilize three
	prominent databases—PostgreSQL, Cassandra, and Redis—as integral components of our evaluation
	framework and execute queries against them. Our methodology involves systematically posing queries
	under different scenarios, adjusting data distribution strategies and the number of workers. We
	measure performance metrics, including query latency and optimizer plans, offering valuable insights
	into how Trino behaves under diverse conditions. This research contributes to understanding Trino's
	efficiency and optimization strategies. The findings provide practical recommendations for optimizing
	data distribution strategies.

\end{abstract}

\begin{IEEEkeywords}
	Big Data, Trino, Distributed SQL Queries
\end{IEEEkeywords}

\section{Introduction}
With the term "Big Data" we refer to data sets whose scale, diversity and complexity require new
architecture, techniques, algorithms and analytics to manage them and extract value from them.
Big data has transformed the way organizations handle information, providing unprecedented
amounts of context and insights. However, big data makes queries slow and is very difficult and
expensive to manage. So, this rapid evolution of big data has forced companies and organizations
to put a lot of effort on  developing new specialised tools designed to manage and analyze those
large data sets in an efficient manner. In response to these challenges, new cutting-edge solutions
emerged, including Trino (formerly known as PrestoDB).

Trino \cite{b3}, is an open-source distributed SQL query engine designed to query large data sets distributed over
one or more heterogenous data sources. Among others, Trino has the ability to run federated queries that
query tables in different data sources such as MySQL, PostgreSQL, Cassandra, Kafka, Redis and many more.
Trino, and its ancestor, PrestoDB, have draught a lot of attention and have now been widely embraced by a high number of
companies, such as Uber \cite{b4}, Twitter \cite{b5}, \cite{b6} and Pinterest \cite{b7}. Therefore, it is significant to evaluate its performance under different
scenarios and circumstances.

In this research, we benchmark the performance of Trino across different types of queries, data locations and underlying
storage technologies. In more detail, we establish a Trino cluster consisting of four nodes, a coordinator and three workers,
where each of them is equipped with a different storage system: PostgreSQL on worker1, Cassandra on worker2 and Redis on worker3.
The diversity among these three database management systems (DBMS) helps us cover three different data models:
Relational, Wide-Column and Key-value providing a more versatile data and query management in our cluster. After, the installation and set up of those three specific stores, we generate and load
data using the well-known TPC-DS \cite{b8} benchmark which is also utilized for query generation.
The size of the data is large enough so that it won't fit in memory, in the order of several GB (milions of records).
The queries imposed to our dataset have a large variety from simple queries (selects) to complex ones (multiple joins and aggregations) while targeting all the dataset tables.
After generating and loading the data, we pose the generated queries into each of the data sources and we come up with some
initial measures. Based on those measures we devise different data distribution strategies and we measure the performance
(query latency, optimizer plan) over different number of workers and different data distribution plans. Our final goal is to
identify how the distributed execution engine and optimization works under a varying amount of data, data distributed in
different engines and with queries of varying difficulty.

This project is backed by a GitHub repository, that contains scripts and howtos on how to replicate our Trino cluster, generate and load the data and
generate and perform the queries. Additionally, the repository includes the results of our measurements, along with a detailed analysis to provide a
thorough understanding of the outcomes.

\section{Related Work}

The need to efficiently process large-scale datasets has driven the development of distributed SQL
query engines, which can handle the challenges posed by diverse data sources and computational
environments. Numerous studies have explored the performance, scalability, and optimization
strategies of these systems.

Cardas et al. \cite{b9} address a gap in research on the performance of SQL-on-Hadoop systems
when deployed in containerized environments using Kubernetes. Their study evaluates four
widely-used SQL platforms—Apache Drill, Apache Hive, Apache Spark SQL, and Trino—deployed
on a Hadoop cluster managed by Kubernetes. Using the TPC-H benchmark, they found that Trino
outperformed the other systems in most query scenarios, ranking highest in performance, followed
by Apache Drill, Apache Spark SQL, and Apache Hive. This research highlights the advantages of
Kubernetes in managing distributed SQL workloads, emphasizing Trino’s superior query execution
times compared to other SQL engines in similar environments. These findings are particularly
relevant to our work, as we also evaluate Trino's performance, although our focus is on different
data distribution strategies and worker configurations rather than the deployment infrastructure.

Similarly, the MuSQLE framework \cite{b10} explores distributed SQL query execution across multiple
engine environments, aiming to optimize performance by leveraging the strengths of different SQL
engines. The study presents a system that dynamically selects the most appropriate execution engine
based on query characteristics and data distribution, thereby enhancing overall query performance
and resource utilization. By comparing various execution strategies, MuSQLE provides insights into
how heterogeneous SQL engines can be orchestrated to handle complex queries efficiently. This approach
complements our research by offering alternative strategies for optimizing distributed SQL queries,
particularly in scenarios involving diverse data sources like PostgreSQL, Cassandra, and Redis.

\section{Source Code}
As mentioned before, the project is backed by a GitHub repository with scripts and howtos which are accessible
through the following link:

\begin{center}
	\textcolor{linkblue}{\underline{\href{https://github.com/alex1on/Information-Systems-NTUA}{GitHub Repository}}}
\end{center}

\section{Technology Stack}

\subsection{Overview}

\begin{itemize}
	\item \textbf{Trino:} Was utilized as a distributed SQL query engine.
	\item \textbf{PostgreSQL:} Database management systems (DBMS) on first worker.
	\item \textbf{Cassandra:} DBMS on second worker.
	\item \textbf{Redis:} DBMS on third worker.
	\item \textbf{TPC-DS Benchmark:} Tool utilized for data and query generation.
\end{itemize}

\subsection{Trino}

\subsubsection{\textbf{\underline{Overview}}}
Trino, formerly known as PrestoDB, is an open-source distributed SQL query engine, build in Java. It is designed to query large amounts of data
distributed over one or more heterogeneous data sources, using distributed queries. It was firstly designed as an alternative to tools that query
Hadoop Distributed File System (HDFS) using pipelines of MapReduce jobs such as Hive or Pig. However, Trino has been extended to operate over different
kinds of data sources, including traditional relational databases such as PostgreSQL and other data sources such as Cassandra. Moreover, it excels in
executing federated queries that span tables in different data sources like MySQL, PostgreSQL, Cassandra, Redis and MongoDB. Furthermore, Trino is
also designed to handle data warehousing and analytics tasks and its functionalitites extend to data analysis, the aggregation of huge data sets and the generation of reports.

\subsubsection{\textbf{\underline{Nodes}}}
As a distributed query engine, Trino processes data in parallel across multiple servers and it runs on a cluster of servers that contains two types of nodes,
a coordinator and a worker \cite{b11} . Users interact with the coordinator using their SQL query tool, and the coordinator collaborates with workers to access connected
data sources. The configuration for this access is stored in catalogs, where parameters like host, port and passwords are defined. Acting as the orchestrator,
the coordinator distributes workloads in parallel across all workers. Each worker node runs Trino in a seperate JVM instance and processing is further parallelized through threads.

The Trino coordinator is responsible for parsing and optimizing statements, planning queries and managing worker nodes and it is the only node to which clients can submit
statements for execution. It maintains oversight of worker activity, coordinates query execution and translates the logical query models into connected tasks. Additionally,
coordinator communicates with clients and workers through a REST API and can be configured to also serve as a worker.

On the other hand, Trino workers are servers responsible for executing tasks and processing data. They fetch data from connectors and exchange intermediate data with
each other. The coordinator is responsible for fetching results from workers and returning the final reuslts to the client. Like coordinator, workers use a REST API
with which they communicate to each other.

\subsubsection{\textbf{\underline{Components}}}
Accessing diverse data sources, executing queries, and delivering results to clients in a distributed manner poses a formidable challenge. Trino addresses this challenge through
the incorporation of various components, with connectors, catalogs, schema, and tables standing out as the most significant and critical among them.

Connectors play a crucial role as they are plugins that enable communication with different data sources and storage systems.
Trino supports various connectors for traditional SQL databases like MySQL, PostgreSQL, Oracle, for NoSQL databases like MongoDB and Cassandra and for modern data lakes like
Hive, Iceberg and Delta Lake. As a result, users can query data from a large variety of different data sources.

Schemas are a way to organize tables. Together, a catalog and schema define a set of tables that can be queried. When accessing Hive or a relational database such as MySQL with Trino,
a schema translates to the same concept in the target database.

A table is a set of unordered rows, which are organized into named columns with types. This is the same as in any relational database. The mapping from source data to tables is defined by the connector.

Catalogs in Trino, are logical containers that hold metadata about the available sources, including schemas, tables and connectors.
Trino can connect to multiple catalogs, each representing a distinct set of data sources and their associated metadata.
Users can switch between catalogs in their queries and access data from different environments without the need to change their SQL statements.

By combining the components mentioned before, Trino gains the ability to represent table structures and query data in SQL, from data sources that do not support the concept of tables or even SQL (such as Redis).

\subsubsection{\textbf{\underline{Architecture}}}

Summarizing the above, a Trino cluster consists of a single node and one or more worker nodes. The coordinator is responsible for admitting, parsing, planning and
optimizing queries as well as query orchestration. Worker nodes are responsible for query processing. Figure 1 illustrates a simplified view of Trino architechture:
\begin{figure}[h]
	\includegraphics[width=0.5\textwidth]{images/trino-architecture.png}
	\caption{Trino Architechture}
	\label{fig:trino_architechture}
\end{figure}


The client sends an HTTP request containing a SQL statement to the coordinator. The coordinator processes the request by parsing and analyzing the SQL text,
creating and optimizing distributed execution plan. The coordinator distributes this plan to workers, starts execution of tasks and then begins to enumerate splits,
which are sections of a larger data set. Splits are assigned to the tasks responsible for reading this data.


% \subsubsection{\textbf{\underline{Query Execution Model}}}

% In Trino's query execution model, SQL statements are executed by transforming them into queries, which are then distributed across the cluster.
% A statement in Trino refers to the textual representation of SQL, while a query refers to the configuration and components instantiated to execute that statement.

% When Trino parses a statement, it converts it into a query and creates a distributed query plan, realized as interconnected stages
% running on Trino workers. The execution process involves stages, tasks, splits, connectors, and other components working together
% to produce a result.

% \begin{itemize}
% 	\item \textbf{Stage:} Trino breaks up query execution into a hierarchy of stages, forming a tree-like structure.
% 	      Stages represent different sections of the distributed query plan and are responsible for aggregating outputs from other stages.
% 	      The coordinator models the plan using stages, but stages themselves do not run on Trino workers.
% 	\item \textbf{Task:} Stages are implemented as a series of tasks distributed over Trino workers.
% 	      Tasks execute in parallel and operate on splits, which are sections of a larger dataset. Tasks are crucial in deconstructing the
% 	      distributed query plan, and they have inputs and outputs. The execution of a stage involves a network of Trino workers working on tasks.
% 	\item \textbf{Split:} Tasks operate on splits, which are sections of the dataset. Splits are obtained from connectors, and during query
% 	      scheduling, the coordinator queries a connector for a list of available splits. The coordinator tracks the machines running tasks and the
% 	      splits being processed.
% 	\item \textbf{Driver:} Tasks contain one or more parallel drivers, acting as the lowest level of parallelism. Drivers process data and combine
% 	      operators to produce output, which is then aggregated and delivered to another task in a different stage.
% 	\item \textbf{Operator:} Operators consume, transform, and produce data. Examples include a table scan fetching data from a
% 	      connector, and a filter operator producing a subset of data by applying a predicate. Operators are building blocks for executing the stages
% 	      of a query.
% 	\item \textbf{Exchange:} Exchanges transfer data between Trino nodes for different stages. Tasks produce data into an output buffer and consume
% 	      data from other tasks using an exchange client. Exchanges facilitate the flow of data between stages in a distributed query plan.
% \end{itemize}

\subsection{PostgreSQL}

PostgreSQL, also known as Postgres, is a highly regarded free and open-source relational database management system (RDBMS) that emphasizes
extensibility and SQL compliance. The system is designed to handle a diverse range of workloads, from single machines to large-scale data warehouses
or web services with numerous concurrent users. PostgreSQL operates seamlessly on major operating systems, including Linux, FreeBSD, OpenBSD, macOS, and Windows.
Some key features that make PostgreSQL very interesting and powerful, are the followings:

\begin{itemize}
	\item \textbf{SQL Compliance and Extensibility:} PostgreSQL allows users to define their own data types, operators, functions, and aggregates. This extensibility
	      provides flexibility for developers to customize and extend the database according to their specific requirements.
	\item \textbf{ACID Properties:} The system ensures data reliability through transactions with Atomicity, Consistency, Isolation and Durability
	      (ACID) properties, enhancing data integrity and consistency.
	\item \textbf{Advanced Functionality:} PostgreSQL supports a rich set of features such as automatically updatable views, materialized views, foreign keys providing
	      flexibility and control over database operations.
	\item \textbf{Concurrency Control:} PostgreSQL employs Multi - Version Concurrency Control (MVCC), which allows multiple transactions to occur simultaneously without interfering with each other. This ensures high concurrency and scalability.
	\item \textbf{Advanced Query Optimization:} The query planner and optimizer in PostgreSQL are sophisticated, enabling efficient execution plans for complex queries.
	      It includes features like join optimization, subquery optimization, and parallel query execution.
\end{itemize}

These features make PostgreSQL the go to selection relational DBMS making it the most suitable selection for a relational database in our cluster.

\subsection{Cassandra}

Cassandra is an open - source, distributed, wide-column store, NoSQL database management system, designed to handle large amounts of data across many commodity servers,
providing high availability with no single point of failure. Some of its main features are listed below:

\begin{itemize}
	\item \textbf{Distributed:} Every node in the cluster has the same role and there is no master. Also there is no single point of failure and data is distributed across the cluster, so
	      each node is responsible for different data.
	\item \textbf{Replication - Fault Tolerance:} Cassandra supports replication across multiple datacenters, providing lower latency for users. Data is automatically replicated to multiple nodes the number
	      of which can be easily configured.
	\item \textbf{Scalability:} Cassandra scales horizontally and is designed in such a way, that allows linear increase in write and read throughput by adding new machines in the cluster.
\end{itemize}

These features, in addition to its efficiency and easy scalability contribute to Cassandra being a very powerful and widely used NoSQL database and makes it a perfect database to experiment on.
% In our project, we also examine Cassandra's scalability so we consider two versions of Cassandra cluster: a single node cluster and a two-node cluster with replication factor of two.

\subsection{Redis}

Redis (Remote Dictionary Server) is an open-source in-memory storage, used as a distributed, in-memory key-value database, cache and message broker, with optional durability. Because it holds all
data in memory and due to its design, Redis offers low-latency reads and writes, making it particularly suitable for cases that require a cache. Additionally, Redis provides a notable speedup to disk
read/write operations and is commonly employed as a cache to a non-volatile database. Moreover, Redis is one of the most popular database systems and
it is frequently used in combination with other databases in order to leverage the strengths of each database for specific use cases. Some key features are listed below:

\begin{itemize}
	\item \textbf{In - Memory Storage:} Redis stores data in-memory, which allows for extremely fast read and write operations.
	\item \textbf{Data Structures:} Redis supports a variety of data structures, including strings, hashes, lists, sets and sorted sets. Each data type comes with its own set of operations.
	\item \textbf{Persistence:} While Redis is an in-memory store, it provides options for persistence. It can be configured to periodically write data to disk, ensuring data durability.
	\item \textbf{Replication:} Redis supports master-slave replication, allowing for the creation of replicas of the master Redis server.
	      This provides high availability and fault tolerance, as the slaves can take over if the master fails.
	\item \textbf{Paritioning:} Redis can be partitioned across multiple nodes, enabling horizontal scaling. This is particularly useful for large datasets and high-throughput scenarios.
\end{itemize}

Redis is utilized in our project in order to store a subset of the data set and compare its efficiency and speed.

\subsection{TPC-DS Benchmark}
\label{sec:tpc-ds-benchmark}

As TPC-DS specification \cite{b8} mentions: "TPC-DS is a decision support benchmark that models several generally applicable
aspects of a decision support system, including queries and data maintenance. The benchmark provides a representative evaluation
of performance as a general purpose decision support system.”

In general, TPC-DS is:

\begin{itemize}
	\item Industry standard benchmark (OLAP / Data Warehouse).
	\item Implemented for many analytical processing systems - RDBMS, Apache Spark, Apache Flink, etc.
	\item It provides a wide range of different SQL queries.
	\item In incorporates the tools to generate input data of different sizes.
\end{itemize}

In our project, TPC-DS is utilized for data and query generation.

\section{Installation and Configuration}

In this section we outline the steps that were followed in order to set up our cluster and replicate our working
environment. We describe the installation and configuration of Trino, the setup of the data sources
on each node, and the process of generating and loading the dataset using the TPC-DS Benchmark.
More in-depth details, instructions and commands can be found in our \textcolor{linkblue}{\underline{\href{https://github.com/alex1on/Information-Systems-NTUA}{GitHub Repository}}}.
\subsection{Cluster and Virtual Machines}
\label{sec:cluster-and-virtual-machines}

For our project we used four virtual machines (1 coordinator and 3 workers) that were provided by ~okeanos-knossos \cite{b12}.
okeanos-knossos is a GRNET'S cloud service (IaaS) for the Greek Research and Academic Community that gives access to resources such as
virtual machines and virtual networks. Each virtual machine has the same following specifications:

\begin{itemize}
	\item \textbf{CPU:} 4 Cores - Intel® Xeon® CPU E5-2650 v3
	\item \textbf{RAM:} 8GB
	\item \textbf{Disk:} 30GB
	\item \textbf{Operating System:} Ubuntu Server 22.04 LTS
\end{itemize}

\subsection{Networking}

For networking we were given one public IPv4 address, which was attached to our master node which also behaves as the Trino coordinator.
In order to allow the communication between the cluster nodes, we set up a local area network (LAN: 192.168.1.0/24). In more details,
192.168.1.1 was assigned to the coordinator, 192.168.1.2 to worker 1 (PostgreSQL), 192.168.1.3 to worker 2 (Cassandra) and 192.168.1.4 to worker 3 (Redis)
as shown in the figure below:

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{images/LAN.png}
	\caption{Cluster's Network Topology}
	\label{fig:LAN}
\end{figure}

After setting up our machines, we used SSH in order to have access to them and proceeded wtih configurations
such as default gateway, firewall rules, hostnames etc. We mention, that okeanos provides Ubuntu Server 16.04 so
we had to upgrade to 22.04.

\subsection{Prerequisites}

Before we begin, we mention the tools that need to be install:

\begin{itemize}
	\item {Java 17.03 or later version}
	\item {Python 3.10.12 or later}
\end{itemize}

\subsection{Trino Cluster Set Up \& Configuration}

For Trino cluster set up we followed the "Deploying Trino" guide from Trino's \textcolor{linkblue}{\underline{\href{https://trino.io/docs/current/installation/deployment.html}{official site}}}.
First of all, we downloaded the latest Trino release (435 at the time being). Next, we created the \texttt{trino-server-435/etc} directory which contains the necessary configuration files:

\begin{itemize}
	\item \textbf{Node properties:} The node properties file, \texttt{node.properties}, contains configuration specific to each node such as the name of the environment and the unique identifier.
	\item \textbf{JVM config:} The JVM config file, \texttt{jvm.config}, contains a list of command line options used for launching the Java Virtual Machine.
	\item \textbf{Config properties:} The config properties file, \texttt{config.properties}, contains the configuration for the Trino server. It provides information such as the port of the HTTP server and the type of node (coordinator, worker or both).
	\item \textbf{Log levels:} The optional log levels file, \texttt{log.properties}, allows setting the desired log level (INFO, DEBUG etc).
\end{itemize}

These files, are present in every node. They also contain the \texttt{trino-server-435/etc/catalog} directory. As mentioned before, Trino accesses data via connectors, which are mounted in catalogs and the connector provides all
of the schemas and tables inside of the catalog. So under this directory, there are several \texttt{.properties} file, one for each used data source, that includes information such as port, host and password,
that allow Trino nodes to access the different data sources.

In order to be able to query the data located in those databases, we downloaded the \textcolor{linkblue}{\underline{\href{https://trino.io/docs/current/client/cli.html}{Trino CLI executable}}}.

\subsection{Database Installation and Connection with Trino}

Following the instructions from the original sites, we install and set up the databases below:

\begin{itemize}
	\item \textbf{PostgreSQL:} We installed PostgreSQL 14 on worker 1.
	\item \textbf{Apache Cassandra:} We installed Cassandra 5.0 on worker 2, following the Debian packages installation guide.
	\item \textbf{Redis:} We installed Redis on worker 3.
\end{itemize}


To ensure accessibility to every database from each node within the Trino cluster and enable querying of the data via Trino CLI, we configured each database as outlined below:

\subsubsection{PostgreSQL}

To expose PostgreSQL to the cluster network on the LAN, we first modified the \texttt{/etc/postgresql/14/main/pg\_hba.conf} file,
adding the IP addresses of the cluster nodes to allow database access. Next, we updated the \texttt{postgres.properties} file with the
correct IP address, username, and password for authentication. Finally, we edited the \texttt{/etc/postgresql/14/main/postgresql.conf}
file, setting the \texttt{listen\_addresses} parameter to the desired LAN IP (192.168.1.2) to permit connections from that address.
\subsubsection{Cassandra}

For Cassandra, we adjusted parameters like authenticator, authorizer, seeds, listen\_address and rpc\_address that are located in \texttt{/etc/cassandra/cassandra.yaml} as follows:

\begin{itemize}
	\item \textbf{authenticator:} class\_name : \\org.apache.cassandra.auth.PasswordAuthenticator
	\item \textbf{authenticator:}
	      class\_name: \\org.apache.cassandra.auth.PasswordAuthenticator
	\item \textbf{rpc\_address, listen\_address and seeds:} 192.168.1.3 (IP address of worker 2 to whom Cassandra is installed)
\end{itemize}

We also configured \texttt{cassandra.properties} accordingly.

\subsubsection{Redis}


In configuring Redis, we made modifications to the \texttt{/etc/redis/redis.conf} file, specifying the worker's 3 IP address in the \texttt{bind} property
to ensure Redis listens to that specific IP. Similar adjustments were made to the \texttt{redis.properties} file as mentioned earlier.

It's important to note that Redis lacks support for schema and table definitions. To address this limitation, Trino tackles the issue by requiring users to
define the schema for each table using specific table schema definition files (in our case, JSON files). Trino can access these files based on their location, which is declared in the \texttt{redis.properties} file.

\subsection{TPC-DS Benchmark installation and data \& query generation}

We download the \textcolor{linkblue}{\underline{\href{https://www.tpc.org/tpc_documents_current_versions/current_specifications5.asp}{TPC-DS benchmark source code}}} and compiled it
utilizing the Makefile that is included in the zip file. Next, we used dsdgen executable to generate a data set of 8GB size and dqgen to generate the 99 queries that the benchmark provides.

\subsection{Data loading}

In order to load the data into the different data sources, we created proper database schemas in PostgreSQL and Cassandra through
tpcds.sql file that creates the tables and \texttt{tpcds\_ri.sql} that specifies foreign key constraints. Since Cassandra and Redis don't support foreign key constraints, they were applied only to PostgreSQL. Our
study is focusing only on querying already existing data and there is no intention to insert, delete or update those data. Since data obbey the constraints in PostgreSQL, we can be sure that the same holds
for Cassandra and Redis.

In Redis we utilized table schema definition files as mentioned earlier. For that purpose, we created \texttt{create\_trino\_json\_table\_definitions.sh} script which is responsible for generating those JSON files.

After creating the schema and tables, we created scripts in order to load the data into the different databases. For PostgreSQL and Cassandra we took advantage of the COPY command that both, SQL and CQL provide.
For Redis we used python modules and loaded the data using hashes.

We note that since Redis is an in-memory database and certain tables contain millions of records, not all records can fit in memory and it wouldn't be efficient to overload RAM. Consequently, the insertion process
focuses on a subset of the data for practical considerations.

In conclusion, we also note, that for our initial experiments, we loaded the complete dataset into both Postgres and Cassandra. Following the initial measurements and theoretical insights, we came up with some early conclusions
which helped up devise a better data distribution strategy as explained later.

\section{Process and Methodology}

\subsection{The TPC-DS Benchmark}

As mentioned in \ref{sec:tpc-ds-benchmark}, the TPC-DS benchmark is a comprehensive tool for evaluating decision support systems,
offering a range of SQL queries and the capability to generate data of varying sizes. In addition to its
technical features, TPC-DS also simulates a realistic business environment, which is crucial for understanding
its applicability in real-world scenarios.

\subsubsection{Business Model}

TPC-DS models industries involved in managing, selling, and distributing products,
ranging from food and electronics to furniture, music, and toys. It reflects the
business structure of a large retail company with a nationwide presence, incorporating
both brick-and-mortar stores as well as catalog and online sales channels. The benchmark
simulates key operational areas such as sales, returns, inventory management, and
promotions. Here are examples of the business processes represented in TPC-DS:

\begin{itemize}
	\item \textbf{} Record customer purchases (and track customer returns) from any sales channel
	\item \textbf{} Modify prices according to promotions
	\item \textbf{} Maintain warehouse inventory
	\item \textbf{} Create dynamic web pages
	\item \textbf{} Maintain customer profiles (Customer Relationship Management)
\end{itemize}

\subsubsection{Logical Database Design}

The TPC-DS schema is carefully crafted to model the sales and returns processes for an organization utilizing three main sales channels:
physical stores, catalogs, and the Internet. At the core of this schema are seven fact tables—inventory, store sales, store returns,
catalog sales, catalog returns, web sales and web returns—which store comprehensive transactional data. Each of these tables is represented
by an ER-diagram that details the relationships between the fact tables and their associated dimension tables. Specifically, the schema
includes paired fact tables dedicated to tracking product sales and returns across the three sales channels, along with a single fact table
for inventory management, particularly for catalog and online sales.

To support these fact tables, the schema incorporates 17 dimension tables that provide contextual details and are shared across all sales channels.
These tables form a snowflake schema, where the central fact tables are connected to dimension tables that may themselves be further normalized.
The logical design of the TPC-DS schema is detailed, with clearly defined tables and relationships, visualized through high-level diagrams that
illustrate the connections in the snowflake schema.

Figures 3 through 9 represent the ER diagrams of each of the seven fact tables, as provided by the TPC-DS Standard Specification \cite{b10}.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{images/ER_Diagrams/Store Sales - ER Diagram.png}
	\caption{Store Sales - ER Diagram}
	\label{fig:store_sales_er_diagram}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{images/ER_Diagrams/Store Returns - ER Diagram.png}
	\caption{Store Returns - ER Diagram}
	\label{fig:store_returns_er_diagram}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{images/ER_Diagrams/Catalog Sales - ER Diagram.png}
	\caption{Catalog Sales - ER Diagram}
	\label{fig:catalog_sales_er_diagram}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{images/ER_Diagrams/Catalog Returns - ER Diagram.png}
	\caption{Catalog Returns - ER Diagram}
	\label{fig:catalog_returns_er_diagram}
\end{figure}


\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{images/ER_Diagrams/Web Sales - ER Diagram.png}
	\caption{Web Sales - ER Diagram}
	\label{fig:web_sales_er_diagram}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{images/ER_Diagrams/Web Returns - ER Diagram.png}
	\caption{Web Returns - ER Diagram}
	\label{fig:web_returns_er_diagram}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{images/ER_Diagrams/Inventory - ER Diagram.png}
	\caption{Inventory - ER Diagram}
	\label{fig:inventory_er_diagram}
\end{figure}

\subsubsection{Query Templates}

The TPC-DS also provides a query suite of 100 queries which we used to benchmark the performance of our different data distribution strategies. According to the TPC-DS specification \cite{b10} the queries can be categorized as follows:

\begin{itemize}
	\item \textbf{Reporting Queries:} Queries that answer pre-defined questions.
	\item \textbf{Ad hoc Queries:}  Dynamic queries - they answer immediate and specific business questions.
	\item \textbf{Iterative OLAP Queries:}  Queries that discover new trends.
	\item \textbf{Data Mining Queries:}  Queries that sift through large data, typically joins and aggregations.
\end{itemize}

\subsection{Data Distribution Strategy}

\subsubsection{No Data Distribution Applied}

Initially, we measured the performance without any data distribution to establish a
baseline system performance value. It is important to note that we did not test
Redis performance on the full data set due to the RAM limitations
(\ref{sec:cluster-and-virtual-machines}) of the virtual machines used in our test system.
Instead, we limited Redis to store only the web sales and returns fact tables along with
their associated dimension tables, in order to compare query run times for queries
involving these (or subsets of these) tables.

Due to the limitations of Redis and the diverse set of queries provided by TPC-DS,
we selected a subset of queries and categorized them into the following groups:

\begin{itemize}
	\item \textbf{Group 1:} Queries involving multiple fact tables (e.g., catalog sales and store sales). The specific queries in this group are: 2, 4, 5, 11, 33, 51, 66, 72, 75, 82, 83, 97.
	\item \textbf{Group 2:} Queries involving a single fact table. The specific queries in this group are: 6, 13, 18, 27, 30, 39, 40, 50, 62, 84, 85, 88, 90, 91, 99.
	\item \textbf{Group 3:} Queries that access tables from the web channel (the only channel that can be fully stored in Redis). The specific queries in this group are: 30, 62, 85, 90.
\end{itemize}

Table \ref{table:query-categorization} shows the categorization of the selected TPC-DS queries into their
respective groups.

We note that, for each benchmark and distribution strategy, we measured the performance 10 times
to ensure more accurate and reliable results.
The full comparison results between PostgreSQL, Cassandra and Redis on the same query group are presented in Section \ref{sec:no-data-distribution}

\subsubsection{Entity-Relationship (ER) Fact Table Distribution with Replicated Dimensions}

The performance measurements from the first test without data distribution (\ref{sec:no-data-distribution})
revealed a clear performance disparity between PostgreSQL, Cassandra, and Redis. PostgreSQL outperformed
the others in most queries, while Redis was significantly slower, as expected, due to its poor optimization
for large datasets (add reference). Cassandra, while generally trailing PostgreSQL, showed better performance
in some queries, though there were significant slowdowns in certain outliers.
%TODO: (insert type of queries that was faster).

Running the base benchmark provided us with a better understanding on each database's performance on
different queries resulting in a clearer view for our following distribution strategies.
The first distribution strategy we tested involves partitioning data based on the entity-relationship
(ER) model of the fact tables.

For this strategy, we focused solely on the queries from \textbf{group 1}, which involve multiple fact tables.
The principles guiding this strategy are as follows:
\begin{itemize}
	\item Fact tables from the same channel are located in the same database (e.g., store\_sales and store\_returns are both in the same database).
	\item Databases are loaded based on the results of the initial test (e.g., larger tables are placed in more performant databases).
	\item Dimension tables that correspond to a fact table are stored in the same database as the fact table (e.g., if inventory is stored in PostgreSQL, then related tables like warehouse, date\_dim, and item are also stored in PostgreSQL).
	\item If a dimension table is associated with multiple fact tables located in different databases, the dimension table is replicated across all relevant databases (e.g., date\_dim, which is related to all fact tables, is stored in all three databases).
\end{itemize}

By following these principles, we aim to achieve two main goals:

\begin{enumerate}
    \item Equally partition data across PostgreSQL, Cassandra, and Redis while considering the results 
	of our base test, which indicated that PostgreSQL should handle the higher load and Redis the lowest.
    \item Exploit data locality by storing fact tables alongside their respective dimension tables, 
	since fact tables are joined with them (according to the ER model), reducing network traffic and optimizing join performance.
\end{enumerate}

This approach balances the data distribution across the three databases while leveraging local joins 
to enhance performance and reduce data movement. Table \ref{table:data-distribution-1} shows the data distribution across databases for this distribution strategy.

\subsubsection{Optimized Fact and Dimension Consolidation Strategy}

In the previous strategy based on the entity-relationship (ER) model of the fact tables, we observed a
performance improvement over Cassandra and Redis but lower performance compared to PostgreSQL alone.
We identified Redis as the primary bottleneck, especially due to its limited ability to handle large
datasets efficiently. Notably, the \texttt{web\_sales} table, the largest fact table stored in Redis,
was a major contributor to this bottleneck.

To address this, we decided to move the \texttt{web\_sales} table from Redis to PostgreSQL.
This change aimed to reduce the load on Redis while leveraging PostgreSQL's ability to handle larger
fact tables more effectively.

Additionally, we wanted to minimize the replication of dimension tables across multiple databases,
which added complexity to query execution. By centralizing frequently joined dimension tables in
PostgreSQL, we could take advantage of Trino’s pushdown capabilities.

\textit{Pushdown optimization} \cite{b13} in Trino allows specific query operations, such as predicates, aggregation
functions, or other operations, to be pushed down to the underlying data source for processing.
This results in several key benefits:
\begin{itemize}
	\item Improved overall query performance.
	\item Reduced network traffic between Trino and the data source.
	\item Lower load on remote data sources.
\end{itemize}

Given that PostgreSQL supports more complex operations, such as joins, better than Cassandra or Redis,
we moved key dimension tables like \texttt{date\_dim}, \texttt{item}, and \texttt{time\_dim} into PostgreSQL.
These tables are frequently used in join operations across many queries, so by centralizing them in PostgreSQL,
we allow the database to handle most of the join workload internally, minimizing the number of records sent
back to Trino for further processing.

By restructuring the data in this way, we aimed to:
\begin{itemize}
	\item Eliminate the bottleneck caused by Redis.
	\item Reduce the complexity of managing replicated dimension tables.
	\item Leverage PostgreSQL’s ability to execute joins and other operations locally, thus improving query
	      performance and reducing the data transferred to Trino.
\end{itemize}

The revised distribution strategy reflects these optimizations, balancing the workload between the databases
while keeping PostgreSQL at the core of handling complex operations.
Table \ref{table:data-distribution-2} depicts the data distribution for this strategy.

\section{Results and Analysis}

In this section we are presenting the results of our experiments that measure the performance of Trino in different settings regarding the data distribution across workers and the configuration of the cluster.

\subsection{Base System Benchmark - No data distribution applied}
\label{sec:no-data-distribution}

As aforementioned, in this first experiment our goal is to establish a base benchmark score between the three databases (PostgreSQL, Cassandra and Redis) while executing the same query in each database from the Trino CLI.

In Figures 10 to 12 we showcase the performance of each database in each query group.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{images/DBs_bench_no_distro/group1.png}
	\caption{PostgreSQL vs Cassandra}
	\label{fig:psql-cass-group1}
\end{figure}

From Figure ~\ref{fig:psql-cass-group1} we conclude that PostgreSQL has on average a 4.5x speedup comparing to Cassandra on queries that consist of multiple big tables (fact tables) with multiple joins (group1 queries). PostgreSQL was expected to be faster in these type of queries considering that Cassandra do not natively support joins and are entirely handled by Trino (add reference here).

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{images/DBs_bench_no_distro/group2.png}
	\caption{PostgreSQL vs Cassandra}
	\label{fig:psql-cass-group2}
\end{figure}

In Figure ~\ref{fig:psql-cass-group2} we compared PostgreSQL and Cassandra on queries that consisted only one fact table and its dimension tables. The queries in this group generally had more frequent aggregations and less frequent joins, especially between large tables. PostgreSQL in this test was only 1.5x faster on average than Cassandra on the same queries. This changes, though, on the second consecutive run where PostgreSQL queries run 3x times faster due to better query caching and dynamic filtering (reference required) compared to Cassandra and Trino.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{images/DBs_bench_no_distro/group3.png}
	\caption{PostgreSQL vs Cassandra vs Redis}
	\label{fig:store_sales_er_diagram}
\end{figure}

Lastly, Redis queries performed on average 22x slower than PostgreSQL and 5.6x slower than Cassandra. This performance penalty is mostly attributed on slower table retrieval from Redis. Redis being a key-store database has no table indexing and is required to use SCAN on all of its records and Trino performs any required filtering on all the initial dataset.

To sum up, our first test showed a clear ranking between the 3 databases, with PostgreSQL performing considerably better than Cassandra with Redis being a distant third.

\subsection{Distribution based on fact tables ER}
\label{sec:distribution-based-on-fact-tables-er}

% TODO: Add performance values

\subsection{2nd distribution strategy}
\label{sec:2nd-distribution-strategy}

\section{Conclusion}
\section{References}
\subsection{Units}
\begin{itemize}
	\item Use either SI (MKS) or CGS as primary units. (SI units are encouraged.) English units may be used as secondary units (in parentheses). An exception would be the use of English units as identifiers in trade, such as ``3.5-inch disk drive''.
	\item Avoid combining SI and CGS units, such as current in amperes and magnetic field in oersteds. This often leads to confusion because equations do not balance dimensionally. If you must use mixed units, clearly state the units for each quantity that you use in an equation.
	\item Do not mix complete spellings and abbreviations of units: ``Wb/m\textsuperscript{2}'' or ``webers per square meter'', not ``webers/m\textsuperscript{2}''. Spell out units when they appear in text: ``. . . a few henries'', not ``. . . a few H''.
	\item Use a zero before decimal points: ``0.25'', not ``.25''. Use ``cm\textsuperscript{3}'', not ``cc''.)
\end{itemize}

\subsection{Equations}
Number equations consecutively. To make your
equations more compact, you may use the solidus (~/~), the exp function, or
appropriate exponents. Italicize Roman symbols for quantities and variables,
but not Greek symbols. Use a long dash rather than a hyphen for a minus
sign. Punctuate equations with commas or periods when they are part of a
sentence, as in:
\begin{equation}
	a+b=\gamma\label{eq}
\end{equation}

Be sure that the
symbols in your equation have been defined before or immediately following
the equation. Use ``\eqref{eq}'', not ``Eq.~\eqref{eq}'' or ``equation \eqref{eq}'', except at
the beginning of a sentence: ``Equation \eqref{eq} is . . .''

\subsection{\LaTeX-Specific Advice}

Please use ``soft'' (e.g., \verb|\eqref{Eq}|) cross references instead
of ``hard'' references (e.g., \verb|(1)|). That will make it possible
to combine sections, add equations, or change the order of figures or
citations without having to go through the file line by line.

Please don't use the \verb|{eqnarray}| equation environment. Use
\verb|{align}| or \verb|{IEEEeqnarray}| instead. The \verb|{eqnarray}|
environment leaves unsightly spaces around relation symbols.

Please note that the \verb|{subequations}| environment in {\LaTeX}
will increment the main equation counter even when there are no
equation numbers displayed. If you forget that, you might write an
article in which the equation numbers skip from (17) to (20), causing
the copy editors to wonder if you've discovered a new method of
counting.

	{\BibTeX} does not work by magic. It doesn't get the bibliographic
data from thin air but from .bib files. If you use {\BibTeX} to produce a
bibliography you must send the .bib files.

	{\LaTeX} can't read your mind. If you assign the same label to a
subsubsection and a table, you might find that Table I has been cross
referenced as Table IV-B3.

{\LaTeX} does not have precognitive abilities. If you put a
\verb|\label| command before the command that updates the counter it's
supposed to be using, the label will pick up the last counter to be
cross referenced instead. In particular, a \verb|\label| command
should not go before the caption of a figure or a table.

Do not use \verb|\nonumber| inside the \verb|{array}| environment. It
will not stop equation numbers inside \verb|{array}| (there won't be
any anyway) and it might stop a wanted equation number in the
surrounding equation.

\subsection{Some Common Mistakes}\label{SCM}
\begin{itemize}
	\item The word ``data'' is plural, not singular.
	\item The subscript for the permeability of vacuum $\mu_{0}$, and other common scientific constants, is zero with subscript formatting, not a lowercase letter ``o''.
	\item In American English, commas, semicolons, periods, question and exclamation marks are located within quotation marks only when a complete thought or name is cited, such as a title or full quotation. When quotation marks are used, instead of a bold or italic typeface, to highlight a word or phrase, punctuation should appear outside of the quotation marks. A parenthetical phrase or statement at the end of a sentence is punctuated outside of the closing parenthesis (like this). (A parenthetical sentence is punctuated within the parentheses.)
	\item A graph within a graph is an ``inset'', not an ``insert''. The word alternatively is preferred to the word ``alternately'' (unless you really mean something that alternates).
	\item Do not use the word ``essentially'' to mean ``approximately'' or ``effectively''.
	\item In your paper title, if the words ``that uses'' can accurately replace the word ``using'', capitalize the ``u''; if not, keep using lower-cased.
	\item Be aware of the different meanings of the homophones ``affect'' and ``effect'', ``complement'' and ``compliment'', ``discreet'' and ``discrete'', ``principal'' and ``principle''.
	\item Do not confuse ``imply'' and ``infer''.
	\item The prefix ``non'' is not a word; it should be joined to the word it modifies, usually without a hyphen.
	\item There is no period after the ``et'' in the Latin abbreviation ``et al.''.
	\item The abbreviation ``i.e.'' means ``that is'', and the abbreviation ``e.g.'' means ``for example''.
\end{itemize}
An excellent style manual for science writers is \cite{b7}.

\subsection{Authors and Affiliations}
\textbf{The class file is designed for, but not limited to, six authors.} A
minimum of one author is required for all conference articles. Author names
should be listed starting from left to right and then moving down to the
next line. This is the author sequence that will be used in future citations
and by indexing services. Names should not be listed in columns nor group by
affiliation. Please keep your affiliations as succinct as possible (for
example, do not differentiate among departments of the same organization).

\subsection{Identify the Headings}
Headings, or heads, are organizational devices that guide the reader through
your paper. There are two types: component heads and text heads.

Component heads identify the different components of your paper and are not
topically subordinate to each other. Examples include Acknowledgments and
References and, for these, the correct style to use is ``Heading 5''. Use
``figure caption'' for your Figure captions, and ``table head'' for your
table title. Run-in heads, such as ``Abstract'', will require you to apply a
style (in this case, italic) in addition to the style provided by the drop
down menu to differentiate the head from the text.

Text heads organize the topics on a relational, hierarchical basis. For
example, the paper title is the primary text head because all subsequent
material relates and elaborates on this one topic. If there are two or more
sub-topics, the next level head (uppercase Roman numerals) should be used
and, conversely, if there are not at least two sub-topics, then no subheads
should be introduced.

\subsection{Figures and Tables}
\begin{table}[htbp]
	\caption{Categorization of Selected TPC-DS Queries}
	\begin{center}
		\begin{tabular}{|c|c|}
			\hline
			\textbf{Group}                         & \textbf{Queries}                                          \\
			\hline
			\textbf{Group 1: Multiple Fact Tables} & 2, 4, 5, 11, 33, 51, 66, 72, 75, 82, 83, 97               \\
			\hline
			\textbf{Group 2: Single Fact Table}    & 6, 13, 18, 27, 30, 39, 40, 50, 62, 84, 85, 88, 90, 91, 99 \\
			\hline
			\textbf{Group 3: Web Channel Tables}   & 30, 62, 85, 90                                            \\
			\hline
		\end{tabular}
		\label{table:query-categorization}
	\end{center}
\end{table}

\begin{table}[htbp]
	\caption{Data Distribution Based on Fact Tables ER}
	\begin{center}
		\begin{tabular}{|l|c|c|c|}
			\hline
			\textbf{Table}          & \textbf{PostgreSQL} & \textbf{Cassandra} & \textbf{Redis} \\
			\hline
			call\_center            &                     & \checkmark         &                \\
			catalog\_page           &                     & \checkmark         &                \\
			catalog\_returns        &                     & \checkmark         &                \\
			catalog\_sales          &                     & \checkmark         &                \\
			customer                & \checkmark          & \checkmark         & \checkmark     \\
			customer\_address       & \checkmark          & \checkmark         & \checkmark     \\
			customer\_demographics  & \checkmark          & \checkmark         & \checkmark     \\
			date\_dim               & \checkmark          & \checkmark         & \checkmark     \\
			household\_demographics & \checkmark          & \checkmark         & \checkmark     \\
			income\_band            & \checkmark          & \checkmark         & \checkmark     \\
			inventory               & \checkmark          &                    &                \\
			item                    & \checkmark          & \checkmark         & \checkmark     \\
			promotion               & \checkmark          & \checkmark         & \checkmark     \\
			reason                  & \checkmark          & \checkmark         & \checkmark     \\
			ship\_mode              &                     & \checkmark         & \checkmark     \\
			store                   & \checkmark          &                    &                \\
			store\_returns          & \checkmark          &                    &                \\
			store\_sales            & \checkmark          &                    &                \\
			time\_dim               & \checkmark          & \checkmark         & \checkmark     \\
			warehouse               & \checkmark          & \checkmark         & \checkmark     \\
			web\_page               &                     &                    & \checkmark     \\
			web\_returns            &                     &                    & \checkmark     \\
			web\_sales              &                     &                    & \checkmark     \\
			web\_site               &                     &                    & \checkmark     \\
			\hline
		\end{tabular}
		\label{table:data-distribution-1}
	\end{center}
\end{table}

\begin{table}[htbp]
	\caption{Optimized Fact and Dimension Consolidation Strategy - Data Parition}
	\begin{center}
		\begin{tabular}{|l|c|c|c|}
			\hline
			\textbf{Table}          & \textbf{PostgreSQL} & \textbf{Cassandra} & \textbf{Redis} \\
			\hline
			call\_center            &                     & \checkmark         &                \\
			catalog\_page           &                     & \checkmark         &                \\
			catalog\_returns        &                     & \checkmark         &                \\
			catalog\_sales          &                     & \checkmark         &                \\
			customer                &                     & \checkmark         &                \\
			customer\_address       &                     & \checkmark         &                \\
			customer\_demographics  &                     & \checkmark         &                \\
			date\_dim               & \checkmark          &                    &                \\
			household\_demographics &                     & \checkmark         &                \\
			income\_band            &                     &                    & \checkmark     \\
			inventory               & \checkmark          &                    &                \\
			item                    & \checkmark          &                    &                \\
			promotion               &                     &                    & \checkmark     \\
			reason                  &                     &                    & \checkmark     \\
			ship\_mode              &                     &                    & \checkmark     \\
			store                   & \checkmark          &                    &                \\
			store\_returns          & \checkmark          &                    &                \\
			store\_sales            & \checkmark          &                    &                \\
			time\_dim               & \checkmark          &                    &                \\
			warehouse               & \checkmark          &                    &                \\
			web\_page               &                     &                    & \checkmark     \\
			web\_returns            &                     &                    & \checkmark     \\
			web\_sales              & \checkmark          &                    &                \\
			web\_site               &                     &                    & \checkmark     \\
			\hline
		\end{tabular}
		\label{table:data-distribution-2}
	\end{center}
\end{table}


\paragraph{Positioning Figures and Tables} Place figures and tables at the top and
bottom of columns. Avoid placing them in the middle of columns. Large
figures and tables may span across both columns. Figure captions should be
below the figures; table heads should appear above the tables. Insert
figures and tables after they are cited in the text. Use the abbreviation
``Fig.~\ref{fig}'', even at the beginning of a sentence.

\begin{table}[htbp]
	\caption{Table Type Styles}
	\begin{center}
		\begin{tabular}{|c|c|c|c|}
			\hline
			\textbf{Table} & \multicolumn{3}{|c|}{\textbf{Table Column Head}}                                                         \\
			\cline{2-4}
			\textbf{Head}  & \textbf{\textit{Table column subhead}}           & \textbf{\textit{Subhead}} & \textbf{\textit{Subhead}} \\
			\hline
			copy           & More table copy$^{\mathrm{a}}$                   &                           &                           \\
			\hline
			\multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
		\end{tabular}
		\label{tab1}
	\end{center}
\end{table}

\begin{figure}[htbp]
	% \centerline{\includegraphics{fig1.png}}
	\caption{Example of a figure caption.}
	\label{fig}
\end{figure}

Figure Labels: Use 8 point Times New Roman for Figure labels. Use words
rather than symbols or abbreviations when writing Figure axis labels to
avoid confusing the reader. As an example, write the quantity
``Magnetization'', or ``Magnetization, M'', not just ``M''. If including
units in the label, present them within parentheses. Do not label axes only
with units. In the example, write ``Magnetization (A/m)'' or ``Magnetization
\{A[m(1)]\}'', not just ``A/m''. Do not label axes with a ratio of
quantities and units. For example, write ``Temperature (K)'', not
``Temperature/K''.

\begin{thebibliography}{00}
	\bibitem{b1} R. Sethi et al., "Presto: SQL on Everything," 2019 IEEE 35th International Conference on Data Engineering (ICDE), Macao, China, 2019,
	\bibitem{b2} Z. Luo et al., "From Batch Processing to Real Time Analytics: Running Presto® at Scale," 2022 IEEE 38th International Conference on Data Engineering (ICDE), Kuala Lumpur, Malaysia, 2022, pp. 1598-1609
	\bibitem{b3} Trino. (2024). Trino | Distributed SQL query engine for big data. Available: \href{https://trino.io/}{https://trino.io/}.
	\bibitem{b4} “Engineering data analytics with Presto and Apache Parquet at Uber,” 2017. [Online]. Available: \href{https://www.uber.com/blog/presto/}{https://www.uber.com/blog/presto/}
	\bibitem{b5} C. Tang et al. “Hybrid-cloud SQL federation system at Twitter,” in ECSA (Companion), 2021.
	\bibitem{b6} “Serving hybrid-cloud SQL interactive queries at Twitter,” in European Conference on Software Architecture. Springer, 2022, pp. 3–21.
	\bibitem{b7} “Presto at Pinterest,” 2019. [Online]. Available: \href{https://medium.com/pinterest-engineering/presto-at-pinterest-a8bda7515e52}{https://medium.com/pinterest-engineering/presto-at-pinterest-a8bda7515e52}
	\bibitem{b8} TPC Benchmark™ DS - Standard Specification, Version 3.2.0, June 2021
	\bibitem{b9} Cristian Cardas, José F. Aldana-Martín, Antonio M. Burgueño-Romero, Antonio J. Nebro, Jose M. Mateos, and Juan J. Sánchez. 2022. On the performance of SQL scalable systems on Kubernetes: a comparative study. Cluster Computing 26, 3 (Jun 2023)
	\bibitem{b10} V. Giannakouris, N. Papailiou, D. Tsoumakos and N. Koziris, "MuSQLE: Distributed SQL query execution over multiple engine environments," 2016 IEEE International Conference on Big Data (Big Data), Washington, DC, USA, 2016
	\bibitem{b11} Ahmad Houwaiji "Trino Deep Dive" 2023. [Online]. Available: \href{https://whitestork.me/blog/13/Trino-Deep-Dive}{https://whitestork.me/blog/13/Trino-Deep-Dive}
	\bibitem{b12} ~okeanos-knossos. [Online]. \href{https://okeanos-knossos.grnet.gr/home/}{https://okeanos-knossos.grnet.gr/home/}
	\bibitem{b13} Pushdown in Trino [Online]. \href{https://trino.io/docs/current/optimizer/pushdown.html}{https://trino.io/docs/current/optimizer/pushdown.html}
\end{thebibliography}

\end{document}
